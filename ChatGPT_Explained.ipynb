{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOS2bEqkihRsqLHcn3OMK+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bghaendler/BJBS-AI-Lab/blob/master/ChatGPT_Explained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot — ChatGPT-3.5\n",
        "\n",
        "ChatGPT is a chatbot that uses language models developed by OpenAI. \n",
        "- The development of ChatGPT is based on earlier models like BERT and GPT-1.0, which use transformer architecture to capture interrelationships within text. \n",
        "- **BERT** uses a masked language model to *extract vectors containing contextual meaning*, while **GPT-1.0** can perform various natural language processing tasks with *little fine-tuning*. \n",
        "- The transformer architecture **uses encoding and decoding to process language**, where the encoder stores language in an understandable form, and the decoder outputs the language."
      ],
      "metadata": {
        "id": "Eu0riibnEwHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Development History\n"
      ],
      "metadata": {
        "id": "BaXX_Hf2FTrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### BERT\n",
        "<img src=\"https://miro.medium.com/max/1400/0*jB2KxBrenMtNCtbu.jpg\">\n",
        "\n",
        "BERT model, which is a language representation model released by **Google in 2018**. \n",
        "- BERT stands for **Bidirectional Encoder Representations from Transformers**. \n",
        "- The model is trained using **masked language modeling and predicting the next sentences**, and it extracts vectors containing contextual meaning and fine-tunes specific tasks. \n",
        "- The paper \"Data Noising As Smoothing In Neural Network Language Models\" https://arxiv.org/abs/1703.02573 is recommended for understanding the masking process. \n",
        "- Each input to BERT has 3 embeddings: \n",
        "  - Token Embedding, \n",
        "  - Segment Embedding, and \n",
        "  - Position Embedding, which are used to distinguish multiple sentences and ensure the order of the sequence. \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/0*Lrv7RlOX-8GPQfza.png\">\n",
        "\n",
        "- **Token Embedding** uses a special token [sentence-level classification, CLS] at the beginning of the sentence, and a **special token** [separator, SEP] at the end of the sentence. \n",
        "- **Segment Embedding** is used to differentiate between multiple sentences, while **Position Embedding** is a lookup table that guarantees the order of the sequence."
      ],
      "metadata": {
        "id": "fBZbEnCIQTD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-1.0**\n"
      ],
      "metadata": {
        "id": "gSFgEaZqGbSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative Pre-trained Transformer (GPT) model, which combines unsupervised pre-training and supervised fine-tuning for NLP tasks. The idea propose to use unsupervised learning to pre-train a generative language model and then fine-tune it using supervised learning for specific downstream tasks, such as sentiment analysis, classification, and textual entailment. \n",
        "\n",
        "The paper presents the learning objectives and concepts, the dataset used for training, and the model architecture and implementation details. \n",
        "\n",
        "The GPT-1 model was trained on the **BooksCorpus** dataset using a 12-layer transformer structure with masked self-attention, and the model's architecture was largely similar to the original transformer work. It was added an auxiliary learning objective during fine-tuning to improve generalization and convergence speed. \n",
        "\n",
        "The model's input sequences were transformed into ordered sequences to make minimal changes to the model architecture during fine-tuning."
      ],
      "metadata": {
        "id": "ac-3o9dMPeVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- OpenAI GPT-1.0 was introduced in June 2018. (OpenAI produced a GPT 1.0 model earlier than BERT)\n",
        "- The main finding is that **combining the transformer architecture with unsupervised pre-training yield promising results**.\n",
        "  - Similarity: BERT and GPT 1.0 are based on an encoder (transformer), which captures the interrelationships within the text\n",
        "  - GPT-1.0 can perform various NLP tasks with **little fine-tuning**.\n",
        "\n",
        "- **Transformer Structure:**\n",
        "\n",
        "![https://miro.medium.com/max/1400/0*E3AhLjX4OXl5xa5X.png](https://miro.medium.com/max/1400/0*E3AhLjX4OXl5xa5X.png)\n",
        "\n",
        "Image from: https://www.dominodatalab.com/blog/transformers-self-attention-to-the-rescue\n",
        "\n",
        "- The concept of encoding and decoding is widely used in various fields. In the field of NLP, there are generally 3 steps to process:\n",
        "1. Accept the language\n",
        "2. Understanding\n",
        "3. Output the language\n",
        "- Language is something that exists explicitly, but how the brain understands, transforms, and store language is something that is still unexplored.\n",
        "- **Encoding** = the process of storing language in an understandable form\n",
        "- **Decoding** = the process of outputting the language\n",
        "- In the language model, the encoder and decoder are formed by each transformer component.\n",
        "\n",
        "![https://miro.medium.com/max/1400/0*Up6SgLfKau4dNkEw.png](https://miro.medium.com/max/1400/0*Up6SgLfKau4dNkEw.png)\n",
        "\n",
        "Image from: https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "- The main difference between BERT and GPT is that BERT only uses the encoder for model training, and GPT only uses the decoder for text generation. GPT-1.0 uses the decoder self-attention transformer to train the mode. The model has 117 million parameters, which gives chances for future models that can better unlock this potential with larger datasets and more parameters"
      ],
      "metadata": {
        "id": "fcAOR-1aI2pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-2.0**\n"
      ],
      "metadata": {
        "id": "S0PQVRCAHMrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The improvements in GPT-2 were mainly accomplished by training the model on a much larger dataset than its predecessor, GPT-1, and by adding more parameters to the model. \n",
        "\n",
        "Two concepts that were discussed in the paper include task conditioning and zero-shot learning. \n",
        "\n",
        "Task conditioning modifies the learning objective to P(output|input, task) so that the model can learn to perform multiple tasks using the same unsupervised model. \n",
        "\n",
        "Zero-shot learning is a special case of zero-shot task transfer where the model understands the task based on the given instruction. \n",
        "\n",
        "The WebText dataset was used to train GPT-2, which contains 40GB of text data from over 8 million documents. \n",
        "\n",
        "GPT-2 had 1.5 billion parameters, which is 10 times more than GPT-1. \n",
        "\n",
        "GPT-2 achieved better performance than GPT-1 on many downstream tasks such as reading comprehension, summarization, and translation."
      ],
      "metadata": {
        "id": "0EB3ITBvQVCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- It was launched in February 2019. **Its dataset is larger than GPT-1.0 but the other is very similar.**\n",
        "- The main difference is that GPT-2.0 can perform well on multiple tasks without being given any training examples.\n",
        "\n",
        "![https://miro.medium.com/max/1400/0*QZxEqY9bSVo3gAU-.png](https://miro.medium.com/max/1400/0*QZxEqY9bSVo3gAU-.png)\n",
        "\n",
        "Image Credit: https://www.mdpi.com/2079-9292/10/21/2706\n",
        "\n",
        "- The concept of **Large language models (LLMs) is used**. To create an extensive, high-quality dataset, the data from **Reddit** and **other platforms were scraped to extract 40GB of web text data to train the model**.\n",
        "\n",
        "![https://miro.medium.com/max/1400/1*xZqMR-7QPF82Y722ZLgd0A.png](https://miro.medium.com/max/1400/1*xZqMR-7QPF82Y722ZLgd0A.png)\n",
        "\n",
        "Image from: https://openai.com/blog/learning-to-summarize-with-human-feedback/\n",
        "\n",
        "- GPT-2.0 was trained on the WebText dataset with 1.5 billion parameters, 10 times that of GPT-1.0.\n",
        "- GPT-2.0 is evaluated on multiple downstream task datasets such as machine translation, text summarization, domain question answering, and reading comprehension."
      ],
      "metadata": {
        "id": "2qH-83lgI35O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GPT-3.0**\n"
      ],
      "metadata": {
        "id": "4eF6ECkyHoEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-3 model, with 175 billion parameters, 10 times more than Microsoft's Turing NLG model and 100 times more than GPT-2. GPT-3.0 was trained on a mix of five different text corpora (structured set of texts).\n",
        "\n",
        "Due to its large capacity and extensive training, GPT-3 can perform tasks in a zero-shot and few-shot setting, such as writing articles and codes, unscrambling words, summarizing numbers, etc.  \n",
        "\n",
        "Although GPT-3 performs well on many NLP tasks, it has limitations such as losing coherency in long sentences and performing poorly on some reading comprehension tasks. \n",
        "\n",
        "It was suggested the need to train bidirectional models at scale to overcome these limitations and approaches like augmentation of learning objective, use of reinforcement learning, etc., to counter its generic language modelling objective. \n",
        "\n",
        "Other limitations include complex and costly inferencing, low interpretability, and uncertainty about how the model works."
      ],
      "metadata": {
        "id": "ddtIumQJQ2G-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- The architecture of GPT-3.0 is basically the same as GPT-2.0. GPT-3.0 has features such as writing articles that are indistinguishable from human-written articles.\n",
        "- It can also perform tasks such as writing SQL queries, writing React & JavaScript codes, and writing an essay.\n",
        "  - **Zero-shot learning** — Uses no examples to learn a task\n",
        "  - **One-shot learning** — Uses past data to learn a task\n",
        "  - **Few-shot learning** — Uses a small set of examples from new data to learn a new task\n",
        "- GPT-3.0 via a public API or application programming interface allows developers to establish new programmatic interactions between applications and users. So that, users can access the world’s most powerful language models with a https://chat.openai.com/chat. This created a paradigm shift in NLP and attracted a large number of beta testers.\n",
        "- It is a dialogue mode (input a sentence, output a sentence)\n"
      ],
      "metadata": {
        "id": "634Mf6vII5IG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **ChatGPT**\n"
      ],
      "metadata": {
        "id": "0N0gK-BgIAxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **ChatGPT introduces “manually labeled data + reinforcement learning”** on the basis of the powerful GPT-3.5 large-scale language model (LLM). \n",
        "  - The purpose of fine-tuning pre-training language models is to understand the meaning of commands such as questions. Different types of commands help LLM learn to **judge what kind of answer is high-quality** (rich in information, helpful to users, harmless, and non-discriminatory) for a user’s question/information.\n"
      ],
      "metadata": {
        "id": "KRG3HTMWI6OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Reinforcement learning with human feedback (RLHF)= “manually labeled data + reinforcement learning”\n"
      ],
      "metadata": {
        "id": "88EJLEryI96w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Under the framework of “manually labeled data + reinforcement learning”, the training process of ChatGPT is divided into the 3 stages:\n",
        "\n",
        "![https://miro.medium.com/max/1400/1*jiPnMtfMns_B3ENB-GGqSQ.png](https://miro.medium.com/max/1400/1*jiPnMtfMns_B3ENB-GGqSQ.png)\n",
        "\n",
        "Image from : https://openai.com/blog/chatgpt/\n"
      ],
      "metadata": {
        "id": "K5IThbIGJDjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Step 1**: Use a supervised learning method to train a GPT-3.5 model (Fine-tuning)\n",
        "  - Refer to the report “**[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)**” pg 33 Table 6,\n",
        "    \n",
        "    - According to InstructGPT’s training data magnitude estimation, the training data is about 10k~20k.\n",
        "    - The **labelers interact with the chatbots and generate multiple rounds of dialogue data**.\n",
        "    - The labelers **give feedback manually once they get dialogue data**. Although its quality is very diverse, **it comes from a real human.**"
      ],
      "metadata": {
        "id": "5TlmEB6vIzuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Step 2:** Collect comparison data according to the quality of responses under the same context (**Reward Model**)\n",
        "    - A large number of questions are randomly selected.\n",
        "    - Multiple different results are provided from the fine-tuning model in Step 1\n",
        "    - The **labelers sort the different results to form a sorted set to pair the trained data “questions and answer”**\n",
        "    - **Learn to Rank (LTR)** which is a sorting method of supervised learning can be divided into 3 types: \n",
        "      - pointwise approach, \n",
        "      - pairwise approach, and \n",
        "      - listwise approach. \n",
        "    - The pairwise approach is used here.\n",
        "    - The basic concept is to compare the samples pairwise, learn to sort from the comparison and establish a comparative relationship between a sorted set for a certain keyword.\n",
        "    - So, it can be used to train the reward model to predict which output the labeler prefers.\n",
        "    - This makes GPT-3.5 shift from command-driven to intention-driven.\n",
        "    - 10K data is enough to train the model because it is used to tell the model the human preferences"
      ],
      "metadata": {
        "id": "cct7sz41JIWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Step 3**: Optimize a policy against the reward model using **[Proximal Policy Optimization (PPO)](https://openai.com/blog/openai-baselines-ppo/)** to fine-tune the initial GPT-3.5 model\n",
        "    - The basic idea is to randomly select a new prompt(question) and score the generated answer from the Reward Model.\n",
        "    - The score is the overall reward value of the answer.\n",
        "    - The score will be sent to the PPO model.\n",
        "    - The **[policy gradient](http://www.scholarpedia.org/article/Policy_gradient_methods#:~:text=Policy%20gradient%20methods%20are%20a,cumulative%20reward)** iterates to update the policy/PPO model parameters several times until the model converges\n",
        "        \n",
        "        ![https://miro.medium.com/max/1400/0*_fSJB6Ds7KO2-AF7.png](https://miro.medium.com/max/1400/0*_fSJB6Ds7KO2-AF7.png)\n",
        "        \n",
        "        Image from: https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/\n"
      ],
      "metadata": {
        "id": "SprhT6-VJKcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **The Successful ChatGPT-3.5 Model**\n",
        "   "
      ],
      "metadata": {
        "id": "56p0SLXGJNIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### The model contains the 4 key points\n",
        " "
      ],
      "metadata": {
        "id": "hY6Jzx-tLZZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "- 1 **InstructGPT (Powerful base model capability)**\n",
        "  - The overall process is basically the same as the ChatGPT process, except for the data collection and base model, and slightly different when initializing the PPO model.\n",
        "  - The data is used to train the model: human-labeled data (instruction + answer, instruction + multiple answers, instruction by users in the process of using the API)\n",
        "  - Instruction + multiple answers may be used for the ChatGPT model.\n",
        "  \n",
        "  ![https://miro.medium.com/max/1400/1*x62pwKya5k-XVt9VWfkS9g.png](https://miro.medium.com/max/1400/1*x62pwKya5k-XVt9VWfkS9g.png)\n",
        "Image from https://openai.com/blog/instruction-following/\n",
        "  \n",
        "  - GPT-3.0’s answer is short and common because GPT-3.0 is used to predict the next word, without considering what answer the user wants\n",
        "  - The InstructGPT **gives answers that the users prefer**.\n",
        "  \n",
        "<img src=\"https://miro.medium.com/max/1400/0*knj1TuE3GM4ZDY1H\">\n",
        "Image from: https://openai.com/blog/instruction-following/\n"
      ],
      "metadata": {
        "id": "hOxKs8dnLc5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **2. Reinforcement learning algorithm with** **Proximal Policy Optimization (PPO)**\n",
        "    - Reinforcement learning can be divided into value-based and policy-based according to the methodology.\n",
        "    - The PPO algorithm is a new type of Policy Gradient algorithm. It is very sensitive to the step size, but it is difficult to choose an appropriate step size. It solves the problem of achieving goals in the environment by optimizing the behavior strategy of the agent.\n",
        "    - If the difference in value between the new and old strategies is too large, it is not conducive to learning.\n",
        "- **3. Large parameter language model**\n",
        "    - More than 175 billion parameters\n",
        "- **4. High-quality real data**\n",
        "    - Labeled dialogue data\n",
        "    - comparative ranking data"
      ],
      "metadata": {
        "id": "Xj4lcuEhLfcK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49aMDZFlHjBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}